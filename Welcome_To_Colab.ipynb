{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vq27kr1iERzF"
   },
   "source": [
    "# MarianMT Translation: English → French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RVn9HDrH6XPs",
    "outputId": "dbc3ccfe-0fca-43cf-ca32-e7e9a1d3f453"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m5tjBkz4HCJ5"
   },
   "source": [
    "### Set Project Directory on Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3hczcnQW7G8U",
    "outputId": "6fc35b92-0c86-44e7-ade6-efe898eed232"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Répertoire actuel : /content/drive/My Drive/en-fon\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "project_path = \"/content/drive/My Drive/en-fon\"\n",
    "os.chdir(project_path)\n",
    "\n",
    "print(\"Répertoire actuel :\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z2aEbwlPHRBy"
   },
   "source": [
    "### Define Languages and File Paths for Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "YTWYtEIU9MmZ"
   },
   "outputs": [],
   "source": [
    "source_lang = \"fr\"\n",
    "target_lang = \"fon\"\n",
    "\n",
    "data_path = \"/content/drive/My Drive/en-fon\"  # à adapter si nécessaire\n",
    "bpe_codes = f\"{data_path}/bpe.codes.4000\"\n",
    "src_vocab = f\"{data_path}/src_vocab.txt\"\n",
    "trg_vocab = f\"{data_path}/trg_vocab.txt\"\n",
    "test_src = f\"{data_path}/test.fr\"\n",
    "test_trg = f\"{data_path}/test.fon\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U5D2o2_3HawA"
   },
   "source": [
    "### Create Directory to Save the Model Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oX03LAxj9QEY",
    "outputId": "f93bacf6-e5d7-4694-c5b8-dedf20e7912f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dossier de sauvegarde : /content/drive/My Drive/masakhane/fr-fon-checkpoint\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "save_path = f\"/content/drive/My Drive/masakhane/{source_lang}-{target_lang}-checkpoint\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "print(\"Dossier de sauvegarde :\", save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uv1lyyUVHyU4"
   },
   "source": [
    "### Load MarianMT Model and Tokenizer from Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ydDLWsyb_ZN6",
    "outputId": "8c2d3065-6096-411d-a42f-ec571d19954e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianTokenizer, MarianMTModel\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-fr\"  # English -> French\n",
    "\n",
    "\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_checkpoint)\n",
    "model = MarianMTModel.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BYVnvxBaIYLj"
   },
   "source": [
    "### Test on model Translation capacity ( Function Using MarianMT Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NatuYXAyAGTU",
    "outputId": "e816c9c8-b7c1-46de-cc61-98a96527c878"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: Hello, how are you today?\n",
      "French: Bonjour, comment allez-vous aujourd'hui ?\n",
      "\n",
      "'The weather is beautiful today.' → 'Le temps est beau aujourd'hui.'\n",
      "\n",
      "'I love learning new languages.' → 'J'adore apprendre de nouvelles langues.'\n",
      "\n",
      "'Machine translation is fascinating.' → 'La traduction automatique est fascinante.'\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianTokenizer, MarianMTModel\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-fr\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_checkpoint)\n",
    "model = MarianMTModel.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Translation function\n",
    "def translate(text):\n",
    "    # Prepare the input\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # Generate the translation\n",
    "    translated = model.generate(**inputs)\n",
    "\n",
    "    # Decode the result\n",
    "    translation = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "\n",
    "    return translation\n",
    "\n",
    "# Test\n",
    "text_en = \"Hello, how are you today?\"\n",
    "text_fr = translate(text_en)\n",
    "print(f\"English: {text_en}\")\n",
    "print(f\"French: {text_fr}\")\n",
    "\n",
    "# Translate multiple sentences\n",
    "texts = [\n",
    "    \"The weather is beautiful today.\",\n",
    "    \"I love learning new languages.\",\n",
    "    \"Machine translation is fascinating.\"\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    print(f\"\\n'{text}' → '{translate(text)}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GhWm1XGUMuth"
   },
   "source": [
    "### Translate Full Paragraphs Using MarianMT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4ym1DBg9AcHS",
    "outputId": "959e1688-923d-4342-b0f9-b7bb331704f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'intelligence artificielle a révolutionné de nombreuses industries. Les algorithmes d'apprentissage automatique peuvent maintenant effectuer des tâches qui étaient autrefois considérées comme exclusivement humaines. L'avenir de l'IA est à la fois passionnant et stimulant.\n"
     ]
    }
   ],
   "source": [
    "# Pour des paragraphes entiers\n",
    "long_text = \"\"\"\n",
    "Artificial intelligence has revolutionized many industries.\n",
    "Machine learning algorithms can now perform tasks that were\n",
    "once thought to be exclusively human. The future of AI is\n",
    "both exciting and challenging.\n",
    "\"\"\"\n",
    "\n",
    "translation = translate(long_text)\n",
    "print(translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9K8nruGcJRrs"
   },
   "source": [
    "### Read and Translate a File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "rQT7x2FjAf1b"
   },
   "outputs": [],
   "source": [
    "def translate_file(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    translation = translate(text)\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(translation)\n",
    "\n",
    "    print(f\"Translation saved in {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EzNjxmLgJtpW"
   },
   "source": [
    "### Interactive English → French Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ViGbNU2tBHj-"
   },
   "outputs": [],
   "source": [
    "print(\"=== English → French Translator ===\")\n",
    "print(\"Type 'quit' to exit\\n\")\n",
    "\n",
    "while True:\n",
    "    text = input(\"English: \")\n",
    "    if text.lower() == 'quit':\n",
    "        break\n",
    "\n",
    "    translation = translate(text)\n",
    "    print(f\"French: {translation}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "enAY6mi1KLTX"
   },
   "source": [
    "### Bidirectional English ↔ French Translation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LykDHYGABZGA",
    "outputId": "a4ed4901-71ab-4876-fd27-323ec596b2cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original (EN): I love Python programming.\n",
      "French: J'adore la programmation de Python.\n",
      "Back to English: I love Python programming.\n"
     ]
    }
   ],
   "source": [
    "model_fr_en = \"Helsinki-NLP/opus-mt-fr-en\"\n",
    "tokenizer_fr_en = MarianTokenizer.from_pretrained(model_fr_en)\n",
    "model_fr_en_loaded = MarianMTModel.from_pretrained(model_fr_en)\n",
    "\n",
    "def translate_fr_to_en(text):\n",
    "    # Prepare input\n",
    "    inputs = tokenizer_fr_en(text, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # Generate translation\n",
    "    translated = model_fr_en_loaded.generate(**inputs)\n",
    "\n",
    "    # Decode result\n",
    "    return tokenizer_fr_en.decode(translated[0], skip_special_tokens=True)\n",
    "\n",
    "# Bidirectional test\n",
    "en_text = \"I love Python programming.\"\n",
    "fr_text = translate(en_text)\n",
    "back_to_en = translate_fr_to_en(fr_text)\n",
    "\n",
    "print(f\"Original (EN): {en_text}\")\n",
    "print(f\"French: {fr_text}\")\n",
    "print(f\"Back to English: {back_to_en}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3TzKbuYKMZlr"
   },
   "source": [
    "## Evaluation of MarianMT Machine Translation Models (EN↔FR) – Speed, Quality, Memory, and Stress Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EyVE1Mu4LVsk",
    "outputId": "be0a7bee-3c43-46b7-ae96-0563249add3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Performance evaluation for model en-fr\n",
      "============================================================\n",
      "Average time: 1.041s ± 0.017s\n",
      "Texts per second: 4.80\n",
      "Milliseconds per text: 208.20ms\n",
      "BLEU score: 87.48\n",
      "chrF score: 92.56\n",
      "\n",
      "Translations comparison:\n",
      "1. Source: Hello, how are you?\n",
      "   Prediction: Bonjour, comment allez-vous ?\n",
      "   Reference:  Bonjour, comment allez-vous ?\n",
      "2. Source: The weather is beautiful today.\n",
      "   Prediction: Le temps est beau aujourd'hui.\n",
      "   Reference:  Le temps est magnifique aujourd'hui.\n",
      "3. Source: I love learning new languages.\n",
      "   Prediction: J'adore apprendre de nouvelles langues.\n",
      "   Reference:  J'adore apprendre de nouvelles langues.\n",
      "4. Source: Machine translation is fascinating.\n",
      "   Prediction: La traduction automatique est fascinante.\n",
      "   Reference:  La traduction automatique est fascinante.\n",
      "5. Source: Thank you very much for your help.\n",
      "   Prediction: Merci beaucoup pour votre aide.\n",
      "   Reference:  Merci beaucoup pour votre aide.\n",
      "\n",
      "Memory usage: 2631.77 MB\n",
      "\n",
      "Stress test:\n",
      "Batch size   1: 1.140s (0.88 texts/s)\n",
      "Batch size   5: 1.547s (3.23 texts/s)\n",
      "Batch size  10: 2.528s (3.96 texts/s)\n",
      "Batch size  20: 4.299s (4.65 texts/s)\n",
      "Batch size  50: 13.236s (3.78 texts/s)\n",
      "\n",
      "============================================================\n",
      "Performance evaluation for model fr-en\n",
      "============================================================\n",
      "Average time: 0.929s ± 0.007s\n",
      "Texts per second: 5.38\n",
      "Milliseconds per text: 185.71ms\n",
      "BLEU score: 88.81\n",
      "chrF score: 94.61\n",
      "\n",
      "Translations comparison:\n",
      "1. Source: Bonjour, comment allez-vous ?\n",
      "   Prediction: Hello, how are you?\n",
      "   Reference:  Hello, how are you?\n",
      "2. Source: Le temps est magnifique aujourd'hui.\n",
      "   Prediction: The weather is beautiful today.\n",
      "   Reference:  The weather is beautiful today.\n",
      "3. Source: J'adore apprendre de nouvelles langues.\n",
      "   Prediction: I love learning new languages.\n",
      "   Reference:  I love learning new languages.\n",
      "4. Source: La traduction automatique est fascinante.\n",
      "   Prediction: Machine translation is fascinating.\n",
      "   Reference:  Machine translation is fascinating.\n",
      "5. Source: Merci beaucoup pour votre aide.\n",
      "   Prediction: Thank you so much for your help.\n",
      "   Reference:  Thank you very much for your help.\n",
      "\n",
      "Memory usage: 2724.78 MB\n",
      "\n",
      "Stress test:\n",
      "Batch size   1: 0.608s (1.64 texts/s)\n",
      "Batch size   5: 1.047s (4.78 texts/s)\n",
      "Batch size  10: 1.312s (7.62 texts/s)\n",
      "Batch size  20: 8.093s (2.47 texts/s)\n",
      "Batch size  50: 7.906s (6.32 texts/s)\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianTokenizer, MarianMTModel\n",
    "import time\n",
    "import torch\n",
    "from sacrebleu.metrics import BLEU, CHRF\n",
    "import numpy as np\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "# ===============================\n",
    "# 1. Set up models and tokenizers\n",
    "# ===============================\n",
    "\n",
    "models = {\n",
    "    \"en-fr\": \"Helsinki-NLP/opus-mt-en-fr\",\n",
    "    \"fr-en\": \"Helsinki-NLP/opus-mt-fr-en\"\n",
    "}\n",
    "\n",
    "tokenizers = {}\n",
    "model_objects = {}\n",
    "\n",
    "for key, checkpoint in models.items():\n",
    "    tokenizers[key] = MarianTokenizer.from_pretrained(checkpoint)\n",
    "    model_objects[key] = MarianMTModel.from_pretrained(checkpoint)\n",
    "    model_objects[key].eval()\n",
    "\n",
    "# ===============================\n",
    "# 2. Translation function\n",
    "# ===============================\n",
    "\n",
    "def translate(texts, model_key=\"en-fr\"):\n",
    "    \"\"\"Translate text or list of texts using specified model.\"\"\"\n",
    "    tokenizer = tokenizers[model_key]\n",
    "    model = model_objects[model_key]\n",
    "\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs)\n",
    "    translations = [tokenizer.decode(t, skip_special_tokens=True) for t in outputs]\n",
    "\n",
    "    return translations if len(translations) > 1 else translations[0]\n",
    "\n",
    "# ===============================\n",
    "# 3. Speed measurement\n",
    "# ===============================\n",
    "\n",
    "def measure_speed(texts, model_key=\"en-fr\", num_runs=3):\n",
    "    times = []\n",
    "    for _ in range(num_runs):\n",
    "        start_time = time.time()\n",
    "        translate(texts, model_key)\n",
    "        end_time = time.time()\n",
    "        times.append(end_time - start_time)\n",
    "\n",
    "    avg_time = np.mean(times)\n",
    "    std_time = np.std(times)\n",
    "    return {\n",
    "        'avg_time': avg_time,\n",
    "        'std_time': std_time,\n",
    "        'texts_per_second': len(texts) / avg_time,\n",
    "        'ms_per_text': (avg_time / len(texts)) * 1000\n",
    "    }\n",
    "\n",
    "# ===============================\n",
    "# 4. Quality measurement (BLEU & chrF)\n",
    "# ===============================\n",
    "\n",
    "def calculate_bleu(predictions, references):\n",
    "    bleu = BLEU()\n",
    "    return bleu.corpus_score(predictions, [references]).score\n",
    "\n",
    "def calculate_chrf(predictions, references):\n",
    "    chrf = CHRF()\n",
    "    return chrf.corpus_score(predictions, [references]).score\n",
    "\n",
    "def evaluate_quality(test_dataset, model_key=\"en-fr\"):\n",
    "    sources = [item[\"source\"] for item in test_dataset]\n",
    "    references = [item[\"reference\"] for item in test_dataset]\n",
    "    predictions = [translate(src, model_key) for src in sources]\n",
    "\n",
    "    bleu_score = calculate_bleu(predictions, references)\n",
    "    chrf_score = calculate_chrf(predictions, references)\n",
    "\n",
    "    return predictions, bleu_score, chrf_score\n",
    "\n",
    "# ===============================\n",
    "# 5. Memory measurement\n",
    "# ===============================\n",
    "\n",
    "def measure_memory(texts, model_key=\"en-fr\"):\n",
    "    model = model_objects[model_key]\n",
    "    tokenizer = tokenizers[model_key]\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        inputs = tokenizer(texts, return_tensors=\"pt\", padding=True).to('cuda')\n",
    "        model.to('cuda')\n",
    "        with torch.no_grad():\n",
    "            model.generate(**inputs)\n",
    "        memory_mb = torch.cuda.max_memory_allocated() / 1024 / 1024\n",
    "        model.to('cpu')\n",
    "    else:\n",
    "        process = psutil.Process(os.getpid())\n",
    "        memory_mb = process.memory_info().rss / 1024 / 1024\n",
    "\n",
    "    return memory_mb\n",
    "\n",
    "# ===============================\n",
    "# 6. Stress test (scalability)\n",
    "# ===============================\n",
    "\n",
    "def stress_test(text, model_key=\"en-fr\", batch_sizes=[1,5,10,20,50]):\n",
    "    results = []\n",
    "    for batch_size in batch_sizes:\n",
    "        texts = [text] * batch_size\n",
    "        start_time = time.time()\n",
    "        translate(texts, model_key)\n",
    "        end_time = time.time()\n",
    "        duration = end_time - start_time\n",
    "        throughput = batch_size / duration\n",
    "        results.append({'batch_size': batch_size, 'time': duration, 'throughput': throughput})\n",
    "        print(f\"Batch size {batch_size:3d}: {duration:.3f}s ({throughput:.2f} texts/s)\")\n",
    "    return results\n",
    "\n",
    "# ===============================\n",
    "# 7. Test datasets\n",
    "# ===============================\n",
    "\n",
    "test_dataset_en_fr = [\n",
    "    {\"source\": \"Hello, how are you?\", \"reference\": \"Bonjour, comment allez-vous ?\"},\n",
    "    {\"source\": \"The weather is beautiful today.\", \"reference\": \"Le temps est magnifique aujourd'hui.\"},\n",
    "    {\"source\": \"I love learning new languages.\", \"reference\": \"J'adore apprendre de nouvelles langues.\"},\n",
    "    {\"source\": \"Machine translation is fascinating.\", \"reference\": \"La traduction automatique est fascinante.\"},\n",
    "    {\"source\": \"Thank you very much for your help.\", \"reference\": \"Merci beaucoup pour votre aide.\"}\n",
    "]\n",
    "\n",
    "test_dataset_fr_en = [\n",
    "    {\"source\": \"Bonjour, comment allez-vous ?\", \"reference\": \"Hello, how are you?\"},\n",
    "    {\"source\": \"Le temps est magnifique aujourd'hui.\", \"reference\": \"The weather is beautiful today.\"},\n",
    "    {\"source\": \"J'adore apprendre de nouvelles langues.\", \"reference\": \"I love learning new languages.\"},\n",
    "    {\"source\": \"La traduction automatique est fascinante.\", \"reference\": \"Machine translation is fascinating.\"},\n",
    "    {\"source\": \"Merci beaucoup pour votre aide.\", \"reference\": \"Thank you very much for your help.\"}\n",
    "]\n",
    "\n",
    "# ===============================\n",
    "# 8. Run evaluations\n",
    "# ===============================\n",
    "\n",
    "for model_key, test_dataset in [(\"en-fr\", test_dataset_en_fr), (\"fr-en\", test_dataset_fr_en)]:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"Performance evaluation for model {model_key}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Speed\n",
    "    sources = [item[\"source\"] for item in test_dataset]\n",
    "    speed = measure_speed(sources, model_key)\n",
    "    print(f\"Average time: {speed['avg_time']:.3f}s ± {speed['std_time']:.3f}s\")\n",
    "    print(f\"Texts per second: {speed['texts_per_second']:.2f}\")\n",
    "    print(f\"Milliseconds per text: {speed['ms_per_text']:.2f}ms\")\n",
    "\n",
    "    # Quality\n",
    "    predictions, bleu, chrf = evaluate_quality(test_dataset, model_key)\n",
    "    print(f\"BLEU score: {bleu:.2f}\")\n",
    "    print(f\"chrF score: {chrf:.2f}\")\n",
    "\n",
    "    print(\"\\nTranslations comparison:\")\n",
    "    for i, (src, pred, ref) in enumerate(zip([item[\"source\"] for item in test_dataset],\n",
    "                                             predictions,\n",
    "                                             [item[\"reference\"] for item in test_dataset]), 1):\n",
    "        print(f\"{i}. Source: {src}\")\n",
    "        print(f\"   Prediction: {pred}\")\n",
    "        print(f\"   Reference:  {ref}\")\n",
    "\n",
    "    # Memory\n",
    "    mem = measure_memory(sources, model_key)\n",
    "    print(f\"\\nMemory usage: {mem:.2f} MB\")\n",
    "\n",
    "    # Stress test\n",
    "    print(\"\\nStress test:\")\n",
    "    stress_test(\"This is a performance test sentence.\", model_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "99KYwYv4BfCM",
    "outputId": "ce94f8a3-4ca1-4aa3-b3cc-bcde7a91bef3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py:3918: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59513]]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle sauvegardé dans ./my_translation_model\n"
     ]
    }
   ],
   "source": [
    "\n",
    "save_path = \"./my_translation_model\"\n",
    "tokenizer.save_pretrained(save_path)\n",
    "model.save_pretrained(save_path)\n",
    "print(f\"Modèle sauvegardé dans {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Welcome To Colab",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
